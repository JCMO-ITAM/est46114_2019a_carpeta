---
title: "Sesion 15 - Selección de Variables Parte 2/3"
author:
-  Juan Carlos Martínez-Ovando
-  Maestría en Ciencia de Datos
date: "Primavera 2019"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    number_sections: yes
    self_contained: yes
    theme: cerulean
    highlight: textmate
fig_align: "center"
fig_width: 18
---

# Seleccion estocastica de variables {.tabset .tabset-fade .tabset-pills}

# Datos

En estas notas ilustramos el procedimiento de **seleccion estocastica de variables** (SSVS por sus siglas en ingles) en el contexto de un modelo lineal generalizado probit.

Los datos corresponden a los de un estudio de identificación de nacimientos prematuros relacionados con la administración de un medicamento general, denotado por `dde`. La asociacion de la identificación de `nacimiento prematuro` y `nacimiento en tiempo`, denotado por la variable $y$, la variable `dde` esta definida bajo la presencia de variables auxiliares, denotadas por cinco variables adicionales (discretas y categoricas). 

Asi, empleamos la siguiente notacion:

* $y \in \{0,1\}$, siendo $1$ si el nacimiento fue `prematuro` y $0$ para el otro caso,

* $x$ representa el nivel/dosis administrada de `dde`

* $z_1, \ldots, z_5$ - variables adicionales.

## Datos simulados

Mostramos los datos a continuacion:

```{r}
if(!require('mvtnorm')){install.packages("mvtnorm")}
library("mvtnorm")

dde <- read.csv('est46114_s17.csv', 
                header = TRUE, sep = ",", 
                quote="\"", dec=".", fill = TRUE)
head(dde)
dim(dde)
summary(dde)
plot(dde)
```

## Estanzarizacion

Y su version **normalizada** para `dde`,

```{r}
dde$x<- (dde$x - mean(dde$x))/sqrt(var(dde$x)) 
summary(dde)
plot(dde)
```

# Modelo  {.tabset .tabset-fade .tabset-pills}

El modelo que consideramos es **probit lineal**, el cual relaciona la probabilidad de $\{y=1\}$ condicional en $w=(x,z_1,\ldots,z_5)$ como
$$
\mathbb{P}\left(y=1|w\right)=\Phi\left(\beta'w\right),
$$
donde $\Phi(\cdot)=N(\cdot|0,1)$ y $\beta=(\beta_x,\beta_1,\ldots,\beta_5)$.

Como vimos en clase, el modelo puede expresarse de manrea extendida empleando variables latentes. En particular, se introduce al modelo la variable latente $z$ que sirve como elemento conector de $\beta'w$ y $\Phi$. Así, el modelo queda expresado como

\begin{eqnarray}
y|z & \sim & Ber\left(\cdot|\theta=Pr(z > 0)\right) \\
z|w & \sim & N(\cdot|\beta'w,1).
\end{eqnarray}

El modelo anterio, incluyendo la variable latente $z$, es no lineal, por lo que estimar $\beta$ no es una tarea sencilla. A continuacion presentamos dos versiones para su estimacion: frecuentista y bayesiana.

# Implementacion  {.tabset .tabset-fade .tabset-pills} 

## Analisis frecuentista

En este caso, se puede obtener un estimador de $\beta$ basada en la maximizacion de la verosimilitud extendida,

$$
lik\left(\beta,(z_i)_{i=1}^n|datos\right) = \prod_{i=1}^{n} Ber\left(y_i|\theta=Pr(z_i > 0)\right)N(z_i|\beta'w_i,1).
$$

La maximizacion de esta funcion se obtiene empleando metodos numericos, en particular el algoritmo EM. La funcion `glm` realiza esta tarea.

```{r}
X <- cbind(1, dde$x,dde$z1,dde$z2,dde$z3,dde$z4,dde$z5)
Y <- dde$y
dde_mle <- glm(Y ~ -1+X, family=binomial("probit"))
summary(dde_mle)

beta_mle <- dde_mle$coef
beta_mle
```

**Que pasaria si incorporamos un termino constante en el predictor lineal?**

##		Analisis bayesiano

En el contexto bayesiano, el `modelo` que describimos anteriormente se complementa con la distribucion sobre nuestro nivel de creencia entorno a los valores plausibles de $\beta$, i.e. la distribucion inicial sobre $\beta$. Dada la linealidad parcial envolucrada en el perdictor lineal, la distribucion inicial es usualmente especificada como una distribucion normal,
$$
\pi(\beta) = N(\beta|m_0,S_0),
$$
donde $m_0$ es el vector inicial esperado de los coeficientes de regresion, mientras que S_0$ denota la covariabvilidad de tales coeficientes.

Empiricamente, es comun especificar $m_0$ y $S_0$ de manera *empirica* (i.e. empleando la informacion de los datos a traves de procedimientos basados en **empirical bayes**, empleando la media y dispersion empírica), o de manera **difusa**, de la siguiente forma

```{r}
beta0 <- rep(0,7)
beta0
Pbeta0 <- 0.25*diag(7)
Pbeta0
```

### MCMC

La distribución final o posterior para el modelo anterio no puede obtenerse de manera analitica cerrada. *Esto es porque el modelo no es conjugado para los parametros, ademas de ser no lineal de manera estructural integrada.*

Inferencia, al igual que el caso frecuentista, descansa en la implementacion de metodos numericos. En particular, metodos *Markov Chain Monte Carlo* (MCMC). Ilustramos la implementacion de estos metodos con el algoritmo *Gibbs sampler*, en el cual se simula una sucesion de parametros y variables latentes simuladas,
$$
\left\{\beta^{(m)},(z_i^{(m)})_{i=1}^n\right\}_{m\geq 1},
$$
la cual tiene como distribucion invariante la distribucion final del modelo,
$$
\pi\left(\beta,(z_i)_{i=1}^n|datos\righ) \propto \left\{\prod_{i=1}^{n} Ber\left(y_i|\theta=Pr(z_i > 0)\right)N(z_i|\beta'w_i,1) \right\} N(\beta|m_0,S_0).
$$
El procedimiento opera como un procedimiento iterativo, fijando:

* Valores inicial de la cadena de Markov

* Numero de iteraciones del MCMC.

```{r}
beta <- rep(0,7)	# Valor inicial de la cadena de Markov
n <- nrow(dde)		# Numero de observaciones/individuos
z <- rep(0,n)		# Valores iniciales de las variables latentes
G <- 1000		# Numero de iteraciones del MCMC
```

Asi, para las iteraciones $m=1,\ldots,M$, se sigue
$$
\beta^{(m)}|(z_i^{(m-1)})_{i=1}^n, datos \sim \pi\left(\beta|(z^{(m-1)}_i)_{i=1}^n|datos\righ),
$$
y 
$$
(z_i^{(m)})_{i=1}^n |\beta^{(m)}, datos \sim \pi\left((z^_i)_{i=1}^n|\beta^{(m)},datos\righ).
$$

```{r}
# 		Gibbs sampler
gt <- 1
for(gt in 1:G){
  eta<- X%*%beta 	# Predictor lineal (variable auxiliar)
  
  #	Muestreo de la distribucion truncada para Z 
  #	de las distribuciones condicionales completas
  z[Y==0] <- qnorm(runif(sum(1-Y), 0,pnorm(0,eta[Y==0],1)), eta[Y==0],1)
  z[Y==1] <- qnorm(runif(sum(Y), pnorm(0,eta[Y==1],1),1), eta[Y==1],1)

  #	Muestreo de la distribucion final completa para beta
  Vbeta <- solve(Pbeta0 + t(X)%*%X)
  Ebeta <- Vbeta%*%(Pbeta0%*%beta0 + t(X)%*%z)
  beta <- c(rmvnorm(1,Ebeta,Vbeta))

  #	Output
  write(t(beta),file='EST46114_SSVS_beta.out', ncol=7, append=T)
  #print(c(gt,round(beta*100)/100))
}
```

### Monitoreo

Trace-plots para `beta[1]` y `beta[2]`

```{r}
par(mfrow=c(2,1))
beta_out<- matrix(scan('EST46114_SSVS_beta.out'), ncol=7, byrow=T)
plot(beta_out[,1],type="l",xlab="iteration",
		ylab="intercept (beta_1)")
plot(beta_out[,2],type="l",xlab="iteration",ylab="slope (beta_2)")
```

### Distribucion final marginal para el coeficiente de pendiente de `DDE`

```{r}
slp <- beta_out[101:1000,2]
par(mfrow=c(1,1))
par(mar=c(5,5,5,5))
plot(density(slp),type="l",xlab="DDE slope (beta_1)",ylab="Posterior Density",
     cex=1.2)
#abline(v=mean(slp))
#abline(v=0.17536139,lwd=2.5) # MLE
#abline(v=0.17536139 + 1.96*0.02909*c(-1,1),lwd=2.5,lty=2)
#abline(v=quantile(slp,probs=c(0.025,0.975)),lty=2)
```

### Curva de respuesta a dosis `DDE` para nacimiento prematuro

```{r}
par(mfrow=c(1,1))
par(mar=c(5,5,5,5))
xg<- seq(-2,2,length=100)  	# Grid
beta<- beta_out[101:1000,] 	# Datos sin periodo de calentamiento
post<- matrix(0,100,4)
for(i in 1:100){
  post[i,1]<- mean(pnorm(beta[,1] + xg[i]*beta[,2]))
  post[i,2:3]<- quantile(pnorm(beta[,1] + xg[i]*beta[,2]),probs=c(0.025,0.975))
  post[i,4]<- pnorm(-1.08068 + xg[i]*0.17536139)
}
xtrue<- xg*sqrt(var(dde$x)) + mean(dde$x)
plot(xtrue,post[,1],xlab="Serum DDE (mg/L)",ylab="Pr(Preterm birth)",cex=1.2,
     ylim=c(0,max(post)), type="l")
lines(xtrue,post[,2],lty=2)
lines(xtrue,post[,3],lty=2)
lines(xtrue,post[,4],lwd=2.5)
```

### Resumen de la distrinbucion final de los coeficientes de regresion

```{r}
table1<- matrix(0,7,5)
for(i in 1:7){
  table1[i,]<- c(mean(beta[,i]),median(beta[,i]),sqrt(var(beta[,i])),
                 quantile(beta[,i],probs=c(0.025,0.975)))
}
table1<- round(table1*100)/100
```

#		Stochastic search variable selection via the Gibbs sampler

```{r}
ddemle<- glm(Y ~ -1+X, family=binomial("probit"))
betamle<- ddemle$coef 		#	MLE
```


**Especificacion inicial**

En el caso de seleccion de variables, se sigue que que algunas variables explicativas pueden ser excluidas debido a:

* **Redundancia de informacion**, debido a que esta puede estar conjuntamente relacionada con el resto de las variables explicativas (todas o un subconjunto de ellas), o

* **Ausencia de relación**, debido a que esta variable pueda no estar correlacionada con la variable de respuesta.

Inspeccionar las combianciones de estos dos escenarios puede ser altamente demandante tanto operativa como computacionalmente.

Los **metodos de regularizacion** como `ridge` y `lasso` permiten resolver este problema.

Alternativamente, desde el punto de vista bayesiano, este problema puede resolverse explicitamente estableciendo un **metodo automatizado de seleccion de variables** (SSVS). Este procedimiento adopta una distribución inicial de la siguiente forma,
$$
\pi(\beta) = \prod_{j=1}^{p}\pi(\beta_j),
$$
donde 
$$
\pi(\beta_j)= \alpha \delta_{\{0\}}(\beta_j)+(1-\alpha)N(\beta_j|m_{j0},s_{j0}),
$$
incluyendo el parametro $0 < \alpha < 1$ como la probabilidad de que $\beta_j=0$ (i.e. la variable $x_j$ es irrelevante para el modelo y los datos).

En el siguiente script se extiendo el Gibbs sampler del procedimiento estandar con la implementacion del procedimiento SSVS.

```{r}
p <- ncol(X)        	# Numero de covariables (predictores)
p0 <- rep(0.5,p)    	# Distribucion inicial para los predictores excluidos
b0 <- rep(0,p)      	# Hiperparametro para el predictor seleccionado
s0 <- rep(2,p)      	# Varianza del componente normal latente
```

**Stochastic search variable selection via data-augmentation**

```{r}
beta <- betamle
n <- nrow(dde)   	# Numero de individuos
z <- rep(0,n)    	# Valores iniciales de variables latentes
G <- 5000        	# Numero de iteraciones MCMC
```

**Stochastic search Gibbs sampler**


```{r}
gt <- 1
for(gt in 1:G){
  #		Data augmentation step
  eta<- X%*%beta 	# Predictor lineal
  
  # Muestreo de la distribucion normal truncada para latentes
  # de la distribucion condicional completa
  z[Y==0]<- qnorm(runif(sum(1-Y),0,pnorm(0,eta[Y==0],1)),eta[Y==0],1)
  z[Y==1]<- qnorm(runif(sum(Y),pnorm(0,eta[Y==1],1),1),eta[Y==1],1)

  # Actualizacion de coeficientes de regresion (uno a la vez, en lugar de por bloque como en el algoritmo anterior)
  j <-1
  for(j in 1:p){
    #	Varianza final condicional para beta_j bajo la distribucion inicial normal
    Vj<- 1/(s0[j]^{-2} + sum(X[,j]^2))
    Ej<- Vj*sum(X[,j]*(z-X[,-j]%*%beta[-j]))
    
	#	Probabilidad condicional de incluir el predictor 'j'
    phat<- 1/(1 + p0[j]/(1-p0[j]) * dnorm(0,Ej,sqrt(Vj))/dnorm(0,b0[j],s0[j]) )          
    m<- rbinom(1,1,phat)
    beta[j]<- m*rnorm(1,Ej,sqrt(Vj))
  }
  
  #	Output
  write(t(beta),file='est46114_s17_beta_ss.out', ncol=7, append=T)
  #print(c(gt,round(beta*100)/100))
}
```

**Trace-plots para beta[1] y beta[2]**

```{r}
beta<- matrix(scan('est46114_s17_beta_ss.out'), ncol=7, byrow=T)
```

En general, hay $2^p$ numero de posibles modelos. Y en nuestro caso particular, estos son $2^p = 2^7 = 128$.


**Grafica de las iteraciones de Gibbs**

```{r}
par(mfrow=c(3,2))
ylb=c("intercept","dde","z1","z2","z3","z4","z5")
for(j in 2:7){
  print(j)
  plot(beta[,j],xlab="iteration",ylab=ylb[j])
}
```

**Grafica de la curva de respuesta a DDE para nacimiento prematuro**

```{r}
par(mfrow=c(1,1))
par(mar=c(5,5,5,5))
xg<- seq(-2,2,length=100)  	# Grid
beta<- beta[1001:nrow(beta),] 	# Periodo de calentamiento
post<- matrix(0,100,4)
for(i in 1:100){
  post[i,1]<- mean(pnorm(beta[,1] + xg[i]*beta[,2]))
  post[i,2:3]<- quantile(pnorm(beta[,1] + xg[i]*beta[,2]),probs=c(0.025,0.975))
  post[i,4]<- pnorm(-1.08068 + xg[i]*0.17536139)
}
xtrue<- xg*sqrt(var(dde$x)) + mean(dde$x)
plot(xtrue,post[,1],xlab="Serum DDE (mg/L)",ylab="Pr(Preterm birth)",cex=1.2,
     ylim=c(0,max(post)), type="l")
lines(xtrue,post[,2],lty=2)
lines(xtrue,post[,3],lty=2)
lines(xtrue,post[,4],lwd=2.5)
```

Resumen de coeficientes de regresion

Incluye:

- media posterior

- mediana

- desviacion estandar

- 95% intervalo de confianza

- Pr(beta_j=0|datos)

```{r}
table1<- matrix(0,7,6)
for(i in 1:7){
  table1[i,]<- c(mean(beta[,i]),median(beta[,i]),sqrt(var(beta[,i])),
                 quantile(beta[,i],probs=c(0.025,0.975)),
		 length(beta[beta[,i]==0,i])/nrow(beta))
}
table1<- round(table1*100)/100
write(t(table1),file='est46114_s17_beta_ss_summary.csv',ncol=6)
```

**Probabilidades finales para los mejores modelos visitados por el algoritmo**

```{r}
Mout <- beta 
Mout <- matrix(as.numeric(I(Mout==0)),nrow(Mout),ncol(Mout)) 
Mindex <- Mout[1,] 		   # Different models sampled starting with first returns 1 if m1 and m2 are the same model
ind<- function(m1,m2){
  as.numeric(all(I(m1==m2)))
}
Im<- apply(Mout,1,ind,m2=Mout[1,]) # Indicadora para las muestras del primer modelo
Nm<- sum(Im)                       # Numero de muestral del primer modelo
Mout<- Mout[Im==0,]
repeat{
  if(length(Mout)==7) Mout<- matrix(Mout,1,7)
  Mindex<- rbind(Mindex,Mout[1,])
  Im<- apply(Mout,1,ind,m2=Mout[1,])
  Nm<- c(Nm,sum(Im))
  print(sum(Nm))
  if(sum(Nm)==nrow(beta)){ 
    break
  } else Mout<- Mout[Im==0,]
}
#	Ordenando los modelos visitados en terminos de sus probabilidades (ordendecreciente)
Pm <- Nm/sum(Nm)
ord <- order(Pm)
Pm <- Pm[rev(ord)]
Mindex <- Mindex[rev(ord),]
table2 <- cbind(Pm,Mindex)
write(t(table2),file='est46114_s17_beta_ss_pmodel.csv',ncol=8)
```