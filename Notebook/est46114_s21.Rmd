---
title: "Sesion 21 - Datos Categoricos y Modelos de Grafos Probabilisticos Parte 4/7"
author:
-  Juan Carlos Martínez-Ovando
-  Maestría en Ciencia de Datos
date: "Primavera 2019"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    number_sections: yes
    self_contained: yes
    theme: cerulean
    highlight: textmate
fig_align: "center"
fig_width: 18
---

\newcommand{\bigCI}{\mathrel{\text{$\perp\mkern-10mu\perp$}}}

# Objetivos

En esta sesion:

* Estudiaremos propiedades inferenciales de los **modelos de grafos probabilisticos** para datos categoricos.

* Implementaremos las soluciones con las librerias `gRbase`, `bayesloglin`:

```
if(!requireNamespace("BiocManager", quietly = TRUE)){install.packages("BiocManager")}
BiocManager::install("graph", version = "3.8")
BiocManager::install("RBGL", version = "3.8")
BiocManager::install("Rgraphviz", version = "3.8")

install.packages("gRbase", dependencies=TRUE)
install.packages("bayesloglin", dependencies=TRUE) 
```


```{r preliminar, include=FALSE}
suppressMessages(library("gRbase"))
suppressMessages(library("graph"))
suppressMessages(library("RBGL"))
suppressMessages(library("Rgraphviz"))

g3 <- ug(~ 1:2 + 2:3:4 + 3:5 + 4:5 + 6)

suppressMessages(library("gRbase"))
data(lizardRAW, package="gRbase")

data <- lizardRAW
colnames(data) <- c("X2","X3","X1")
data <- data[,c("X1","X2","X3")]
data.tc <- table(data)
data.tc <- table(data)
data.prob <- table(data)/sum(table(data))
```

# Modelos loglineales

> Nota:- Las siguientes definiciones hacen referencia al `ejemplo previo`.

Los `modelos loglineales`, representan las probabilidades $\theta_v$ de una tabla de contingencia $T_v$ de manera `logaditiva`, como
$$
\log \theta_v \propto \alpha^0,
$$
donde $\alpha^0$ es un factor unico comun para todas las `dimensiones` en $v$ y todas sus `categorias`.

Otra alternativa, corresponderia a especificar,
$$
\log \theta_v \propto \alpha^{1}_{e(1)} + \alpha^{2}_{e(2)} + \alpha^3_{e(3)},
$$
donde $\alpha^j_{e(j)}$ corresponde a un factor particular para la `dimension` $j$ en la `categoria $e(j)$`, con $j=1,\ldots,J=3$.

De manera complementaria, podriamos especificar las `probabilidades` de la siguiente forma:
$$
\log \theta_v  \propto \alpha^0 + \alpha^{12}_{e(1,2)} + \alpha^{23}_{e(2,3)} + \alpha^{13}_{e(1,3)},
$$
donde $\alpha^{jk}_{e(j,k)}$ corresponde a un factor particular para las `dimensiones` $\{jk\}$, con $j,k=1,\ldots,J=3$, y sus correspondientes `categorias` $\{e(j),e(k)\}$.


> Asi, podemos tener `multiples representaciones` para $\log \theta_{v}$ asociadas con una misma tabla de contingencia $T_v$.

## Estructura

> La estructura en el `modelo loglineal` se obtiene estableciendo los terminos de interaccion en $0$ siguiendo el principio de que si un termino de interaccion se establece en $0$, todos los terminos de orden superior que contienen esos terminos de interaccion tambien deben establecerse en $0$.

### Ejemplo

En el caso del `ejemplo previo`, consideremos que $\alpha^{2,3}_{e(2,3)}=0$, entonces todos los factores que contengan $\{e(1),e(2)\}$ deben ser iguales a $0$, i.e.
$$
\alpha^{1,2,3}_{e(1,2,3)}=0,
$$
para $$e(1,2,3)=\left(e(1),e(2),e(3)\right).$$

## Generadores

Los `modelos loglineales` pueden especificarse en terminos de sus `generadores` (`factores individuales` y `factores de interaccion` distintos de $0$).

Por ejemplo, respecto al `ejemplo previo`, si:

* definimos $\alpha^{2,3}_{e(2,3)}=0$ y 

* definimos $\alpha^{1,2,3}_{e(1,2,3)}=0$,

tenemos que los `generadores` pueden listarse como
$$
\{ \{1\}, \{2\}, \{3\}, \{(1,2)\}, \{(1,3)\} \}=^\mathbb{P}\{\{X_1\},\{X_2\},\{X_3\},\{X_{1,2}\},\{X_{1,3}\}\}.
$$

### Modelo saturado

El `modelo saturado` se define como la representacion de $\log \theta_v$ correspondiente a no tener factores $a$s iguales a cero, i.e. en el `ejemplo previo`,
$$
\log \theta_v = \alpha^0 + \sum_{j}\alpha^j_{e(j)} + \sum_{jk} \alpha^{j,k}_{e(j,k)} + \alpha^{1,2,3}_{e(1,2,3)}.
$$

Los `generadores del modelo saturado`, por definicion, son 
$$
\{ \{1\}, \{2\}, \{3\}, \{(1,2)\}, \{(2,3)\}, \{(1,3)\}, \{(1,2,3)\}\}.
$$

> Nota:- El `modelo saturado` es, por construccion, no identificable estructuralmente hablando.

### Modelo independiente

El `modelo independiente` se define como el `modelo loglineal` para el cual todos los factores de interaccion entre `dimensiones/variables` son iguales a cero, i.e. en el `ejemplo previo`
$$
\log \theta_v = \alpha^0 + \sum_{j} \alpha^j_{e(j)}.
$$

En este caso, los generadores corresponden a las `dimensiones/variables` marginales $$\{1,2,3\}.$$

> Nota:- El `modelo independiente` considera solamente las `contribuciones marginales` de cada `dimension`.

## Grafo probablistico

La `tabla de contingencia`, $T_v$, vista como un `grafo de probabilidad`, representara las `proabilidades` $\theta_v$ de acuerdo al `grafo inducido` $$\mathcal{G}(V,E),$$
donde 

* El conjunto $V$ corresponde a los `vertices/dimensiones` y 

* El conjunto $E$ corresponde a los `bordes` inducidos por los `generadores` del `modelo loglineal correspondiente`.

De esta manrea buscaremos representar las `probabilidades` de la siguiente forma
$$
\theta_{v}=\prod_{G\in \mathcal{C}_G}\phi_{A},
$$
donde 
$$
\mathcal{C}_G=\{G_1,\ldots,G_Q\},
$$
es la `clase generadora` del `modelo loglineal` correspondiente, siendo $Q$ el numero de elementos en $\mathcal{C}_G$.

### Modelo independiente

Por ejemplo, retomando el `ejemplo previo`, y considerando el `modelo independiente` tenemos que la `clase generadora` corresponde a 
$$
\mathcal{C}_G=\left\{ \{1\}, \{2\}, \{3\} \right\},
$$
i.e. $Q=3$, con $G_1=\{1\}$, $G_2=\{2\}$ y $G_3=\{3\}$.

## Factorizacion

A partir de la descompocion de $\theta_v$, podremos contrastar o aplicar el `teorema de factorizacion` de grafos:

> Consideremos que $\mathcal{G}(V,E)$ define un `grafo de probabilidades` $\left(\theta_v\right)_{v\in V}$ con $\theta_v=\prod_{G\in \mathcal{G}_G}\phi_G$, con $\mathcal{G}_C$ inducida por $E$. Si existen subconjuntos $A$, $B$ y $S$ de $V$ tales que $$\theta_v = \phi_{A,S} \times \phi_{B,S},$$ diremos que $$A\bigCI B|S.$$

Es decir, las `variables/dimensiones` en $A$ y en $B$ seran condicionalmente indeppendientes dado $S$.

# Inferencia

Hasta el momento, hemos revisado:

* Teoria relevante de `grafos matematicos`

* Especificaciones de `grafos probabilisticos`, y su conexion con los `modelos loglineales`.

Todo esto para encontrar `especificaciones` de las probabilidades $\theta_v$s de una tabla de contingencia $T_v$.

Recordemos que las `probabilidades`, cuando estas son **desconocidas**, estan referidas a cada una de las observaciones, i.e. la tabla de contingencia $T_v$ del `ejemplo previo` se calcula como:

```{r}
data.prob
```

Esta corresponde a una `traba de contingencia` de `probabilidades muestrales`.

De manera general abstracta, leemos que para una `unidad de observacion` general `X_{i,v}` la `probabilidad` de que adopte la `configuracion`
$$
x_{i,v} = \left(x_{i,1}=\text{anoli},x_{2,1}=\text{<=4}, x_{i,3}=\text{=>4.75}\right),
$$
 es
 $$
 \mathbb{P}(X_{i,v}=x_{i,v})=\theta_{e(1),e(2),e(3)}.
 $$
$$
\begin{eqnarray}
 e(1) & = & \text{"anoli"}, \\
 e(2) & = & \text{"<=4"}, \\
 e(3) & = & \text{"<=4.75"}. \\
\end{eqnarray}
$$
 
* En el caso muestral, esta probabilidad la calculamos como
$$
\theta_{e(1),e(2),e(3)}  = 0.07823961.
$$

> Generalmente, los valores $(\theta_v)_{v\in V}$ son `desconocidos`. Esto, considerando que las $\theta_v$s con `probabilidades no muestrales`. 

> Los valores $(\theta_v)_{v\in V}$ `desconocidos` se convierten en los parametros de un modelo `multinomial`, como comentamos en algun momento.

## Parametrizacion

Asi, el `grafo probabilistico` asociado con el `grafo` $\mathcal{G}(V,E)$ es parametrizado por  el conjunto
$$
\{\theta_{v}:v\in V\},
$$
con 
$$
\theta_{v} \geq 0,
$$
y 
$$
\sum_{v}\theta_v = 1.
$$

Es decir, el `espacio parametral` del `grafo probabilistico` es el simplejo de dimension $\left(\prod_{j} K_j -1\right)$.

## Verosimilitud

Asi, la evidencia provista por un conjunto de `datos/configuraciones` de `n` objetos es representada por 
$$
\begin{eqnarray}
lik\left(\theta_v|x_1,\ldots,x_n\right) 
  & = & \prod_{i=1}^{n}\theta_{e(i)} \\
  & = & \prod_{v\in V} \left( \theta_{v} \right)^{n_v},
\end{eqnarray}
$$
con $$e(i)=\left(e(1),\ldots,e(J)\right),$$ la configuracion correspondiente a la observacion $i$, donde
$$
n_v = \left\{x_i: x_{i}=x_v\right\}
$$
donde $x_v$ es la `configuracion` asociada con los valores especificos $e_v$.

### Tablas de frecuencias

La `tabla de contingencia`, en este caso, la podemos recuperar de dos formas:

1. `tabla de contingencia`
```{r}
data.tc
```

2. `tabla de frecuencias`
```{r}
data.tcf <- read.csv("./est46114_s21_data.csv", header=TRUE)
data.tcf
sum(data.tcf$freq)
```

### Estimacion puntual

Es facil obtener que el `estimador puntual` de los parametros $\left(\theta_v\right)_{v\in V}$ a partir de los `datos` (i.e. `frecuencias`) dados por $\left(n_v\right)_{v\in V}$ se obtienen como
$$
\hat{\theta}_v = \frac{n_v}{n},
$$
para todo $v\in V$.

> Los estimadores puntuales $\left(\hat{\theta}_v\right)_{v\in V}$ tienen asociada `correlacion negativa`, inducida por la estructura composicional de la `tabla de contingencia`. Por tal, cuantificar `intervalos de confianza/credibilidad` (a.k.a. `regiones de confianza/credibilidad`) es complicado.

> Los estimadores anteriores, dependen del `generador` del `grafo probabilistico` de un `modelo loglineal` especifico.

### Objetivo

> Ahora estudiaremos como calcular la `incertidumbre epistemica` de varios `modelos de grafos probabilisticos`, simultaneamente, y, con base en esto, `comparar` y `seleccionar` las `especificaciones mas relevantes` para un conjunto de datos.

## Multiples modelos

> Anteriormente vimos que la tabla de contingencia la podemos representar en terminos de `frecuencias` y `probabilidades`.

Ahora, partiremos de su representacion como una `tabla de frecuencias esperadas`, i.e. si 
$$
\mathbb{E}(T_v) = \left(\lambda_v\right)_{v \in V}, 
$$
es decir, 
$$
\lambda_v = \mathbb{E}(X_v),
$$
para todo $v\in V$. Sujeto a 
$$
\sum_{v}\lambda_v = N,
$$
donde $N$ es el tamano de la poblacion,

> En la especificacion anterior, $N$ puede ser `conocido` o `desconocido/aleatorio`.

La especificacion anterior presupone que condicional en $N$, la `frecuencia absoluta` de cada `configuracion` cumple con 
$$
N_v=n_v|N \sim \text{Po}(n_v|\lambda_v),
$$
para todo $v\in V$.

Con base en esto, podemos asociar el `grafo probabilisitico` con los `modelos loglineales`, descomponiendo las `tasas` $\lambda_v$ como
$$
\log \lambda_v= \sum_{G\in \mathcal{C}_G} \alpha_{v(G)},
$$
i.e. el logaritmo de la `tasa de incidencia esperada` en cada celda de la `tabla de contingencia` la podemos expresar en terminos de los factores asociados con un `generador especifico` $\mathcal{C}_G$.

### Multiples generadores

> En la practica, no podemos `garantizar` un `generador especifico` antes de observar los datos. Pero si podemos proponer muchos generadores,$$\mathcal{C}_{G,1},\ldots,\mathcal{C}_{G,M},$$ de manera que los `datos` calculen `evidencia de plausibilidad` para cada uno de ellos.

La `evidencia de plausibilidad` de multiples generadores con un conjunto de datos especifico se define en terminos de la `verosimilitud del generador`, i.e.
$$
lik\left(\mathcal{C}_{G,m}|\text{datos}\right) \propto \int lik\left((\lambda_v)_{v\in V(\mathcal{C}_{G,m})}|\text{datos}\right) \pi\left((\lambda_v)_{v\in V(\mathcal{C}_{G,m})}\right)\text{d}(\lambda_v)_{v\in V(\mathcal{C}_{G,m})},
$$
para $m=1,\ldots,M$. 

La verosimilitud anterior se conoce como `verosimilitud integrada`.

las probabilidades de cada uno de los `generadores` se calculan como
$$
\mathbb{P}\left(\mathcal{C}_{G,m}|\text{datos}\right) =\frac{\omega_m lik\left(\mathcal{C}_{G,m}|\text{datos}\right)}{\sum_{m'=1}^{M}\omega_m' lik\left(\mathcal{C}_{G,m'}|\text{datos}\right)}.
$$
> En la practica, no es necesario calcular $\mathbb{P}\left(\mathcal{C}_{G,m}|\text{datos}\right)$
explicitamente, a menos que la `prior` sobre el `espacio de generadores`, $(\omega_m)_{m=1}^M$ sea `no ambigua/no difusa`.

### Calculo, comparacion y seleccion

En `R` la estimacion de las `verosimilitudes` y `probabilidades` se puede calcular con la libreria `bayesloglin`, de la siguiente forma:

```{r}
suppressMessages(library("bayesloglin"))
model.graf <- MC3 (init = NULL, 
                alpha = 1, 
                iterations = 5000, 
                replicates = 1,
                data = data.tcf, 
                mode = "Graphical")
```

Con base en estos calculo, podemos ranquear los `generadores` especificos mas importantes (i.e. con `mayor probabilidad`):

```{r}
head(model.graf, n = 5)
```

Los resultados inferenciales para el `mejor modelo`, correspondiente al generador
$$
\mathcal{C}^*=\left\{\{1,2\},\{1,3\}\right\},
$$
son:

```
formula <- freq ~ X1*X2 + X1*X3

model.opt <- gibbsSampler(formula, alpha = 1, 
                       data = data.tcf,
                       nSamples = 15000, 
                       verbose = T)

# Media
postMean <- colSums(model.opt[5000:15000,]) / 10000

postMean

# Covarianza
postCov <- cov(model.opt[5000:15000,])

postCov

# Varianza
postVar <- diag(data.postCov)

postVar
```

# Lecturas complementarias

* Nota personal sobre tablas de contingencia $2\times 2$. `est46114_s19_suplemento1.pdf` 

* Coppi, *An Introduction to Multi-Way Data and Their Analysis.* `est46114_s19_suplemento2.pdf` 
 