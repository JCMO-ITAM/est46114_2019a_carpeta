---
title: "Sesión 11: Corrección de Tarea del Análisis de Componentes Principales"
author: "Alejandra Lelo de Larrea Ibarra"
date: "24 de febrero de 2019."
header-includes:
  - \usepackage[spanish,mexico]{babel}
  - \usepackage{arydshln}
  - \usepackage{lscape}
  - \usepackage{rotating}
  - \usepackage{xcolor}
  - \usepackage{float}
  - \usepackage{framed}
output: 
  pdf_document:
    fig_caption: yes
    number_sections: true
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/ITAM_Maestria/04_Primavera_2019/02_EstMultivar_DatosCat/03_Ejercicios")
```


# Introducción 

Se tienen datos con los tipos de cambio reales (respecto a USD) de varias economias (periodo 1970-2010). El objetivo es implementar el procedimiento inferencial PCA considerando distribuciones iniciales no informativas para ($\mu$, $\Lambda$) y contestar lo siguiente: 

* ¿Qué economía tiene el mayor peso esperado en la descomposicion PCA?

* ¿Qué economía tiene la mayor consistencia en estimacion de los cjs correspondientes?


# Datos

```{r, message=FALSE,warning=FALSE}
# Se cargan los paquetes
library("fields")
library("mnormt")
library("MCMCpack")
library("actuar")
library("ggplot2")
library("kernlab")
library("tidyverse") 
library("readr") 
library("psych") 
library("mvtnorm")
library("MASS")
library("xlsx")
library("knitr")
```

```{r funciones_auxilares}
# Función para extraer modas
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}
```

Leemos los datos correspondientes a los tipos de cambio de distintas economías. Se tienen 492 observaciones mensuales para 80 economías. 

```{r datos}
# Cargamos los datos)
data<-read.xlsx("../01_Notas_Ovando/est46114_s06_data.xls",sheetName = 'RealXR_Data')

# Obtenemos las dimensiones de los datos
dim(data)

# Extraemos las fechas 
fechas<-data$Date

data<-select(data,-Date)
```

## Análisis exploratorio

Vemos que países están en la muestra: 
```{r paises}
# Vemos la lista de países 
colnames(data)

```

Graficamos las series de tiempo de los países para tener una idea de qué esté pasando. 

```{r seriesTiempo,fig.width=12,fig.height=14}
# SE parte el plot en 8 pedazos
par(mfrow=c(4,2))

# Se grafican series de tiempo de los tipos de cambio
ts.plot(as.ts(data[,1:10],start=c(1970,1),end=c(2010,12),frequency=12))
ts.plot(as.ts(data[,11:20],start=c(1970,1),end=c(2010,12),frequency=12))
ts.plot(as.ts(data[,21:30],start=c(1970,1),end=c(2010,12),frequency=12))
ts.plot(as.ts(data[,31:40],start=c(1970,1),end=c(2010,12),frequency=12))
ts.plot(as.ts(data[,41:50],start=c(1970,1),end=c(2010,12),frequency=12))
ts.plot(as.ts(data[,51:60],start=c(1970,1),end=c(2010,12),frequency=12))
ts.plot(as.ts(data[,61:70],start=c(1970,1),end=c(2010,12),frequency=12))
ts.plot(as.ts(data[,71:80],start=c(1970,1),end=c(2010,12),frequency=12))

```


Se puede ver que los tipos de cambio siguen trayectorias muy distintas. Exploremos más a fondo un par de éstas: 

## México vs India 

```{r}
# Encontramos las columnas correspondientes a México y a la India
cols<-match(c('Mexico','India'),colnames(data))

# Extraemos los datos 
data2<-data[,cols]

# Graficamos las series de tiempo por serparado 
par(mfrow=c(1,2))
par(mar=c(5.1,2.1,1.6,1.6))
plot(as.ts(data2$Mexico,start=c(1970,1),end=c(2010,12),frequency=12),ylab="",xaxt='n',xlab="",main="Mexico")
axis(1,at=seq(1,length(fechas),24),labels=fechas[seq(1,length(fechas),24)],las=2,cex.axis=0.8)
plot(as.ts(data2$India,start=c(1970,1),end=c(2010,12),frequency=12),ylab="",xaxt='n',xlab="",main="India")
axis(1,at=seq(1,length(fechas),24),labels=fechas[seq(1,length(fechas),24)],las=2,cex.axis=0.8)
```

Veamos la FAC y la FACP de las series de tiempo

```{r}
par(mfrow=c(1,2))
acf(data2[,1])
acf(data2[,2])
```

ambas series presentan una fuerte autocorrelación de orden alto. Esto indica que sí hay dependencia entre los renglones de los datos. 


De esta manera, como se puede notar, las dos series no parecen cumplir con los supuestos establecidos para realizar ACP que son: 

* Media constante por renglón (estas dos series no tienen la misma media a lo largo del tiempo)

* Varianza constante por renglón (la volatilidad de los tipos de cambio ha variado a lo largo del tiempo, sobre todo para México)

* Homogeneidad en los renglones (si se cambia el orden de las observaciones no obtiene la misma estructura para las series de tiempo.)

* Simetría estocástica (lo que pase entre 2 renglones no es ajeno entre sí ya que existe autorcorrelación)


Tratemos de extraer la media por año y centrar los datos con este valor para estabilizar la series. 
```{r}
# Se agrega la variable del año 
data2$year<-as.numeric(format(fechas,"%Y"))

# Se extraen las medias por año 
data_means<-data2%>%group_by(year)%>%summarise(Mean.Mex=mean(Mexico),Mean.India=mean(India))

# Se agregan las medias dependiendo del año
data2<-left_join(data2,data_means, by="year")

# Se restan las medias 
data2<-data2%>%
  mutate(Mex.Est=Mexico-Mean.Mex,
         India.Est=India-Mean.India)

# Graficamos los datos centrados por año
par(mfrow=c(1,2))
par(mar=c(5.1,2.1,1.6,1.6))
plot(as.ts(data2$Mex.Est,start=c(1970,1),end=c(2010,12),frequency=12),ylab="",xaxt='n',xlab="",main="Mexico")
axis(1,at=seq(1,length(fechas),24),labels=fechas[seq(1,length(fechas),24)],las=2,cex.axis=0.8)
plot(as.ts(data2$India.Est,start=c(1970,1),end=c(2010,12),frequency=12),ylab="",xaxt='n',xlab="",main="India")
axis(1,at=seq(1,length(fechas),24),labels=fechas[seq(1,length(fechas),24)],las=2,cex.axis=0.8)
```

Como podemos ver, ya se solucionó el hecho de que la serie oscile alredor del cero, sin embargo seguimos teniendo problemas con la volatilidad de las mismas. Apliquemos la misma estrategia estandarizando los datos con la desviación estándar de cada año. 

```{r}
# Se extraen las desviaciones estándar por año 
data_sd<-data2%>%
  select(year,Mexico, India)%>%
  group_by(year)%>%
  summarise(sd.Mex=sd(Mexico),sd.India=sd(India))

# Se agregan las desviaciones estándar dependiendo del año
data2<-left_join(data2,data_sd, by="year")

# Se restan las medias 
data2<-data2%>%
  mutate(Mex.Est2=(Mexico-Mean.Mex)/sd.Mex,
         India.Est2=(India-Mean.India)/sd.India)

# Graficamos los datos centrados por año
par(mfrow=c(1,2))
par(mar=c(5.1,2.1,1.6,1.6))
plot(as.ts(data2$Mex.Est2,start=c(1970,1),end=c(2010,12),frequency=12),ylab="",xaxt='n',xlab="",main="Mexico")
axis(1,at=seq(1,length(fechas),24),labels=fechas[seq(1,length(fechas),24)],las=2,cex.axis=0.8)
plot(as.ts(data2$India.Est2,start=c(1970,1),end=c(2010,12),frequency=12),ylab="",xaxt='n',xlab="",main="India")
axis(1,at=seq(1,length(fechas),24),labels=fechas[seq(1,length(fechas),24)],las=2,cex.axis=0.8)
```

Ahora si, las medias de las variables parecen ser constantes en el tiempo y su variabilidad también. De esta manera tenemos dos opciones: 

* modificar los datos para que se cumplan los supuestos 

* modificar los supuestos del modelo para que se adecúen a los datos. 


En esta ocasion, vamos a modificar los supuestos del análisis de componentes principales para que se adecúen a los datos. 

# Inferencia de SPCA

Sea $x_{m,t}$ una observación de los datos, donde $m=1,2,...12$ y $t=1970,1971,...,2010$. 

Supuestos: 

* $\mu_t=E[X_{mt}]$ tal que $m=1,2,...,12$. 

* $\Sigma_t=var[X_{mt}]$ tal que $m=1,2,...12$.  

* $X_{mt,\cdot} \sim N_p(\boldsymbol{X}|\boldsymbol{\mu_t},\boldsymbol{\Lambda_t})$ donde $\boldsymbol{\mu_t}$ y $\boldsymbol{\Lambda_t}$ son desconocidos $\forall t=1970,...,2010$. 


Con base en estos supuestos, esto es equivalente a aplicar un ACP por año para los datos en la muestra. Vamos a estandarizar todas las variables por año. 

## Estandarización de los datos 

```{r}
# Extrae el año de cada observación
data$year=as.numeric(format(fechas,"%Y"))

# Se extraen las medias por año por variable
data_mean<-data%>%
  group_by(year)%>%
  summarise_all(mean)

colnames(data_mean)[2:ncol(data_mean)]<-paste("mean",colnames(data_mean)[2:ncol(data_mean)],sep=".")


# Se extraen las desviaciones estándar por año por variable
data_sd<-data%>%
  group_by(year)%>%
  summarise_all(sd)

colnames(data_sd)[2:ncol(data_sd)]<-paste("sd",colnames(data_sd)[2:ncol(data_sd)],sep=".")

# Funcion para estandarizar datos. 
estandarizar<-function(country){
  
  aux<-select(data,year,contains(country))
  aux.mean<-select(data_mean,year,contains(country))
  aux.sd<-select(data_sd,year,contains(country))
  
  aux<-left_join(aux,aux.mean,by="year")%>%
    left_join(aux.sd,by="year")
  
  colnames(aux)<-c("year","obs","mean","sd")
  
  aux<-aux%>%
    mutate(estand=(obs-mean)/sd)
  
  return(aux$estand)
}

# Se estandariza cada uno de los tipos de cambio con media y varianza anual. 
data_estand<-lapply(colnames(data)[1:(ncol(data)-1)],estandarizar)

# Se asignan nombres a los elementos de la lista 
names(data_estand)<-colnames(data)[1:(ncol(data)-1)]

# Se convierte a dataframe y se elimina nicaragua.
data_estand<-as_data_frame(data_estand)%>%select(-Nicaragua)
```

Nota: la serie de tiempo de Nicaragua tiene faltantes en el inicio de la muestra que están llenados como ceros, por lo tanto generan errores al estandarizar. Se elimina nicaragua de la muestra. 


## Actualizacion bayesiana de parámetros

El desconocimiento acerca de $(\boldsymbol{\mu_t},\boldsymbol{\Lambda_t})$ se expresa como
$$
(\boldsymbol{\mu_t},\boldsymbol{\Lambda_t}) \sim \text{N-Wi}(\boldsymbol{\mu_t},\boldsymbol{\Lambda_t}|\boldsymbol{m}_{0t},s_{0t},a_{0t},\boldsymbol{B}_{0t}),
$$

donde $\boldsymbol{m}_{0t}=0,s_{0t}=1/2,a_{0t}=1,\boldsymbol{B}_{0t}=I$; es decir, se tiene una distribución inicial no informativa.

De esta manera, la distribución posterior de $(\mu_t,\Lambda_t)$ a partir de la consolidación de la información contenida en los datos y la información complementaria está dada por

$$
(\boldsymbol{\mu_t},\boldsymbol{\Lambda_t}|\boldsymbol{X_t}) \sim \text{N-Wi}(\boldsymbol{\mu_t},\boldsymbol{\Lambda_t}|\boldsymbol{m}_{nt},s_{nt},a_{tn},\boldsymbol{B}_{nt}),
$$
con
\begin{eqnarray*}
\boldsymbol{m}_{nt}&=&\dfrac{s_{0t}m_{0t}+n\bar{x}}{s_{nt}},\\
&&\\
s_{nt}&=&s_{0t}+n,\\
&&\\
a_{nt}&=&a_{0t}+\dfrac{n}{2},\\
&&\\
\boldsymbol{B}_{nt}&=&\boldsymbol{B}_{0t}+\dfrac{n}{2}\left[S+\dfrac{s_{0t}}{s_{nt}}(\bar{x}-m_{0t})(\bar{x}-m_{0t})'\right]
\end{eqnarray*}


```{r posterior, include=TRUE}

# Fijar hiperparámetros
  a0 <- 1
  s0 <- 1/2
  m0 <- matrix(0,ncol=1,nrow=ncol(data_estand))
  B0 <- diag(1,ncol=ncol(data_estand),nrow=ncol(data_estand))
  
  
# Función para calcular la posterior
gaussian.posterior <- function(data,m0,s0,a0,B0){
  
  # Media de los datos
  xbar <- as.matrix(colMeans(data))
  
  # Mat. de var y cov de los datos.
  S <- cov(data)
  
  # No. de obs.
  n <- nrow(data)
  
  # No. de variables.
  p <- ncol(data)
  
  # Parámetros actualizados de la posterior.
  sn <- s0 + n
  an <- a0 + n/2
  mn <- (s0*m0 + n*xbar)/sn
  Bn <- B0 + (n/2)*(S + (s0/sn)*(xbar-m0)%*%t(xbar-m0))
  
  # Salida (parámetros actualizados)
  output <- list(mn=mn,sn=sn,an=an,Bn=Bn)
  return(output)
}

# Calculamos los hiperparams actualizados para la posterior
output <- gaussian.posterior(data_estand,m0,s0,a0,B0)
  

```

## Simulación

Una vez realizada la actualización bayesiana de los hiperparámetros, simulamos $M$ observaciones para ($\mu_t$, $\Lambda_t$) como
\begin{eqnarray}
\boldsymbol{\Lambda_t}^{(m)} & \sim & \text{Wi}(\boldsymbol{\Lambda_t}|a_{nt},\boldsymbol{B}_{nt}), \nonumber \\
\boldsymbol{\mu_t}^{(m)}|\boldsymbol{\Lambda_{nt}}^{(m)} & \sim & \text{N}(\boldsymbol{\mu_t}|\boldsymbol{m}_{nt},s_{nt}\boldsymbol{\Lambda_t}^{(m)}). \nonumber
\end{eqnarray}

Con estos parámetros, obtenemos simulaciones de los eigenvalores y eigenvectores $(e_{jt}^{(m)},\boldsymbol{v}^{(m)}_{jt})_{j=1}^{p},$ de la matriz de varianzas y covarianzas de los datos simulada en el paso anterior (inverso de $\Lambda_t^{(m)}$.

Por último, obtenemos simulaciones de las componentes principales como $\boldsymbol{c}_{jt,\cdot}^{(m)}=\boldsymbol{X}\boldsymbol{v}_{jt}^{(m)}$.

```{r simulacion_alternativa,eval=FALSE, include=FALSE}

# Función para simular los datos
simulacion_acp<-function(data){
  
  # Fijar hiperparámetros
  a0 <- 1
  s0 <- 1/2
  m0 <- matrix(0,ncol=1,nrow=ncol(data))
  B0 <- diag(1,ncol=ncol(data),nrow=ncol(data))
  
  # Calculamos los hiperparams actualizados para la posterior
  output <- gaussian.posterior(data,m0,s0,a0,B0)
  
  # Se fija el no de simulaciones
  M <- 10000
  
  # Matriz para guardar medias
  mu.sim <- matrix(NA,nrow=M, ncol=ncol(data))
  
  # Arreglo para guardar precisiones
  Lambda.sim <- array(NA,dim=c(M,ncol(data),ncol(data)))
  
  # Matriz para guardar valores propios.
  e.sim <- matrix(NA,nrow=M, ncol=ncol(data))
  
  # Arreglo para gaurdar vectores propios
  V.sim <- array(NA,dim=c(M,ncol(data),ncol(data)))
  
  # Arreglo para guardar componentes principales
  C.sim <- array(NA,dim=c(M,nrow(data),ncol(data)))
  
  # Se convierten los datos a matriz.
  X <- as.matrix(data)
  
  # En cada iteración:
  for(m in 1:M){
    
    # Se simulan valores (mu,Lambda)
    Lambda.sim[m,,] <- rWishart(1, output$an, output$Bn)
    mu.sim[m,] <- mvrnorm(1, mu=output$mn, Sigma=solve(output$sn*Lambda.sim[m,,]), tol = 1e-6)
    
    # Simulacion de eigenvalores y eigenvectores (e,V)
    eigen_aux <- eigen(solve(Lambda.sim[m,,]))
    e.sim[m,] <- eigen_aux$values
    V.sim[m,,] <- eigen_aux$vectors
    
    # Simulación de componentes principales.
    C.sim[m,,] <- X %*% V.sim[m,,]
  }
  
  return(list(Lambda.sim=Lambda.sim,
              mu.sim=mu.sim,
              e.sim=e.sim,
              V.sim=V.sim,
              C.sim=C.sim))
}

```

```{r simulacion}

# Función para simular los datos

  # Se fija el no de simulaciones
  M <- 10000
  
  # Matriz para guardar medias
  mu.sim <- matrix(NA,nrow=M, ncol=ncol(data_estand))
  
  # Arreglo para guardar precisiones
  Lambda.sim <- array(NA,dim=c(M,ncol(data_estand),ncol(data_estand)))
  
  # Matriz para guardar valores propios.
  e.sim <- matrix(NA,nrow=M, ncol=ncol(data_estand))
  
  # Arreglo para gaurdar vectores propios
  V.sim <- array(NA,dim=c(M,ncol(data_estand),ncol(data_estand)))
  
  # Arreglo para guardar componentes principales
  C.sim <- array(NA,dim=c(M,nrow(data_estand),ncol(data_estand)))
  
  # Se convierten los datos a matriz.
  X <- as.matrix(data_estand)
  
  # En cada iteración:
  for(m in 1:M){
    
    # Se simulan valores (mu,Lambda)
    Lambda.sim[m,,] <- rWishart(1, output$an, output$Bn)
    mu.sim[m,] <- mvrnorm(1, mu=output$mn, Sigma=solve(output$sn*Lambda.sim[m,,]), tol = 1e-6)
    
    # Simulacion de eigenvalores y eigenvectores (e,V)
    eigen_aux <- eigen(solve(Lambda.sim[m,,]))
    e.sim[m,] <- eigen_aux$values
    V.sim[m,,] <- eigen_aux$vectors
    
    # Simulación de componentes principales.
    C.sim[m,,] <- X %*% V.sim[m,,]
  }
  


```




# Resultados

## Eigenvalores

```{r,fig.height=12,fig.width=12}
multi.hist(e.sim)
```

Todas las distribuciones de las simulaciones correspondientes a los valores propios se ven mucho más homogenes y simétricas que antes. 


## ¿Qué economía tiene el mayor peso esperado en la descomposición PCA?

Los pesos de las componentes están representados por los eigenvectores. Utilizamos la moda de la distribución para obtener la matriz de eigenvectores promedio.

```{r PesosMax}
# Estimador puntual de la matriz de eigenvectores.
V.mode<-matrix(NA,ncol(data_estand),ncol(data_estand))

# Se extrae la moda para cada elemento de la matriz de eigenvectores
for(i in 1:ncol(data_estand)){

  for(j in 1:ncol(data_estand)){

    V.mode[i,j]<-getmode(V.sim[,i,j])
  }
}

colnames(V.mode)<-paste("PC",1:ncol(data_estand),sep="_")
rownames(V.mode)<-colnames(data_estand)

# Se busca el peso más grande en cada una de las componentes.
peso_max<-data_frame(CP=colnames(V.mode),
                     Economia=apply(V.mode,2,function(x){rownames(V.mode)[which.max(x)]}))

kable(cbind(peso_max[1:20,],
            peso_max[21:40,],
            peso_max[41:60,],rbind(peso_max[61:79,],rep(NA,2))),
      type='latex',
      caption='Economia con mayor peso por componente principal')

# Tabla de frecuencias para pesos por país
frec_peso_max<-table(peso_max$Economia)
frec_peso_max<-data_frame(Economia=names(frec_peso_max),
                         Frecuencia=frec_peso_max)%>%
  arrange(desc(Frecuencia))

if(nrow(frec_peso_max)%%2==0){
kable(cbind(frec_peso_max[1:ceiling(nrow(frec_peso_max)/2),],
            frec_peso_max[(ceiling(nrow(frec_peso_max)/2)+1):nrow(frec_peso_max),]),
      type='latex',
      caption='Frecuencias de Economías con Mayor Peso')
}else{
  kable(cbind(frec_peso_max[1:ceiling(nrow(frec_peso_max)/2),],
            rbind(frec_peso_max[(ceiling(nrow(frec_peso_max)/2)+1):nrow(frec_peso_max),],rep(NA,2))),
      type='latex',
      caption='Frecuencias de Economías con Mayor Peso')
}

```

Los resultados son completamente diferentes a los obtenidos sin estandarizar las variables con sus medias y varianzas anuales. Ahora las simulaciones son mucho más estables. 


## ¿Qué economía tiene la mayor consistencia en estimacion de los cjs correspondientes?

```{r}
# Varianza de los eigenvectores.
V.var<-matrix(NA,ncol(data_estand),ncol(data_estand))

# Se calcula la varianza de cada elemento de los eigenvectores
for(i in 1:ncol(data_estand)){

  for(j in 1:ncol(data_estand)){

    V.var[i,j]<-var(C.sim[,i,j],na.rm=TRUE)
  }
}

colnames(V.var)<-paste("PC",1:ncol(data_estand),sep="_")
rownames(V.var)<-colnames(data_estand)

# Se busca el menor varianza en cada una de las componentes.
var_min<-data_frame(CP=colnames(V.var),
                     EcoVarMin=apply(V.var,2,function(x){rownames(V.var)[which.min(x)]}))

kable(cbind(var_min[1:20,],var_min[21:40,],var_min[41:60,],rbind(var_min[61:79,],rep(NA,2))),
      type='latex',
      caption='Economia con Menor Varianza por Componente Principal')


# Tabla de frecuencias para pesos por país
frec_var_min<-table(var_min$EcoVarMin)
frec_var_min<-data_frame(Economia=names(frec_var_min),
                         Frecuencia=frec_var_min)%>%
  arrange(desc(Frecuencia))

kable(frec_var_min,
      type='latex',
      caption='Frecuencia de Economías con menor varianza')

```

Disminuyó el número de paises con mayor inconsistencia en los componentes principales. Esto es gracias a que se estandarizan los datos por variable. 
